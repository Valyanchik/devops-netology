# Домашнее задание к занятию "5.5. Оркестрация кластером Docker контейнеров на примере Docker Swarm"


## Задача 1

Дайте письменые ответы на следующие вопросы:

- В чём отличие режимов работы сервисов в Docker Swarm кластере: replication и global?

В режиме global количество инстансов одного сервиса устанавливается равным количеству нод в кластере  
В режиме replication можно устанавливать вручную нужное количество инстансов  

- Какой алгоритм выбора лидера используется в Docker Swarm кластере?  
Используется алгоритм RAFT  

- Что такое Overlay Network?  
Overlay-сеть создает подсеть, которую могут использовать контейнеры в разных хостах swarm-кластера. Контейнеры на разных физических хостах могут обмениваться данными по overlay-сети (если все они прикреплены к одной сети)  

## Задача 2
Создать ваш первый Docker Swarm кластер в Яндекс.Облаке  

По аналогии с предыдущим ДЗ создаем новый folder, сеть\подсеть в яндекс-облаке:  

```bash
valyan@valyan-pc:~$ yc init
Welcome! This command will take you through the configuration process.
Pick desired action:
 [1] Re-initialize this profile 'sa-profile' with new settings 
 [2] Create a new profile
 [3] Switch to and re-initialize existing profile: 'default'
 [4] Switch to and re-initialize existing profile: 'netology'
Please enter your numeric choice: 4
Please go to https://oauth.yandex.ru/authorize?response_type=token&client_id=1a6990aa636648e9b2ef855fa7bec2fb in order to obtain OAuth token.

Please enter OAuth token: **********(сюда вставляем токен яндекс-облака)
You have one cloud available: 'cloud-valentinos94' (id = b1g1tl9752nijen8v4mi). It is going to be used by default.
You have no available folders. You will be guided through creating one.
Please enter a folder name: netology
Your current folder has been set to 'netology' (id = b1gbi7kvrh79af09o7qd).
Do you want to configure a default Compute zone? [Y/n] y
Which zone do you want to use as a profile default?
 [1] ru-central1-a
 [2] ru-central1-b
 [3] ru-central1-c
 [4] Don't set default zone
Please enter your numeric choice: 1
Your profile default Compute zone has been set to 'ru-central1-a'.
```

После реинициализации профиля с указанными параметрами выводим получившиеся значения:  
```bash
valyan@valyan-pc:~$ yc config list
token: *****************
cloud-id: b1g1tl9752nijen8v4mi
folder-id: b1gbi7kvrh79af09o7qd
compute-default-zone: ru-central1-a
```

Создаем сеть и подсеть:  
```bash
valyan@valyan-pc:~$ yc vpc network create \
> --name net \
> --labels my-label=netology \
> --description "network for docker swarm image"
id: enpn91e733n3451181it
folder_id: b1gbi7kvrh79af09o7qd
created_at: "2022-05-19T09:34:35Z"
name: net
description: network for docker swarm image
labels:
  my-label: netology
valyan@valyan-pc:~$ yc vpc subnet create --name my-subnet-a --zone ru-central1-a --range 10.1.2.0/24 --network-name net --description "subnet via yc"
id: e9b8s6n1gr2it4ks9neg
folder_id: b1gbi7kvrh79af09o7qd
created_at: "2022-05-19T09:36:10Z"
name: my-subnet-a
description: subnet via yc
network_id: enpn91e733n3451181it
zone_id: ru-central1-a
v4_cidr_blocks:
- 10.1.2.0/24

valyan@valyan-pc:~$ 
```
По итогам смотрим, что получилось в веб-интерфейсе:
![img.png](img.png) созданные папка, сеть и подсеть


СОздаем образ ОС на основе скрипта при помощи пакера, проверяем скрипт и созданный образ в яндексоблаке:  

```bash
valyan@valyan-pc:~$ packer validate centos-7-base.json
The configuration is valid.
valyan@valyan-pc:~$ cat centos-7-base.json
{
  "builders": [
    {
      "disk_type": "network-nvme",
      "folder_id": "b1gbi7kvrh79af09o7qd",
      "image_description": "by packer",
      "image_family": "centos",
      "image_name": "centos-7-base",
      "source_image_family": "centos-7",
      "ssh_username": "centos",
      "subnet_id": "e9b8s6n1gr2it4ks9neg",
      "token": "********************",
      "type": "yandex",
      "use_ipv4_nat": true,
      "zone": "ru-central1-a"
    }
  ],
  "provisioners": [
    {
      "inline": [
        "sudo yum -y update",
        "sudo yum -y install bridge-utils bind-utils iptables curl net-tools tcpdump rsync telnet openssh-server"
      ],
      "type": "shell"
    }
  ]
}
```
В скрипте меняем значения token, folder_id, subnet_id  
Запускаем сборку образа предварительно проверив корректность файла  
Весь вывод  процесса сборки в терминале не прикрепляем (не зачем нагромождать) 
```bash
valyan@valyan-pc:~$ packer build centos-7-base.json
yandex: output will be in this color.

.......
==> Builds finished. The artifacts of successful builds are:
--> yandex: A disk image was created: centos-7-base (id: fd8bg7u7kjpqun35p7a1) with family name centos
```
Образ собрался успешно, проверяем наличие образа в консоли:   

```bash
valyan@valyan-pc:~$ yc compute image list
+----------------------+---------------+--------+----------------------+--------+
|          ID          |     NAME      | FAMILY |     PRODUCT IDS      | STATUS |
+----------------------+---------------+--------+----------------------+--------+
| fd8bg7u7kjpqun35p7a1 | centos-7-base | centos | f2e99agij1uhrk2ioilk | READY  |
+----------------------+---------------+--------+----------------------+--------+

valyan@valyan-pc:~$ 
```
В качестве дополнительной проверки смотрим в веб-интерфейсе список образов:  
![img_1.png](img_1.png)


```bash
valyan@valyan-pc:~/terraform$ cat key.json
{
   "id": "aje8oc341p57qsu92ue2",
   "service_account_id": "ajephgumq7ijagfatoo0",
   "created_at": "2022-05-19T10:07:09.753352717Z",
   "key_algorithm": "RSA_2048",
   "public_key": "-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA0xdwjPWvlZL29oOAvTZF\n0FOCbto5THFVuwk/jYC9bjg4I85sabJQCYPja3xco/KKoorURrUqq3pVSYAcADak\nkycLx460YFWu3+cYXDh/ew9JoEIVFoYuOP0p+JKRRphONvIaAahXisZwlcfaN3yY\nHnNs9SMaegrt1QgWqyHPynXDMswY7bSot1PiIePCjMW21LcdXKD3x/D+LZHt3iWy\n1EG4RTTAL8TSyZuSfBzw/oh5/cWf5XwJSYNH2KKHVuO6MBrVSyBvcEoRPIz4YDoi\n/CjNojikZ8Zjwao8s7cUwvS32lR37bKfqopiiErS86imLPNuWvC5/+vZP7l7Om3y\neQIDAQAB\n-----END PUBLIC KEY-----\n",
   "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDTF3CM9a+Vkvb2\ng4C9NkXQU4Ju2jlMcVW7CT+NgL1uODgjzmxpslAJg+NrfFyj8oqiitRGtSqrelVJ\ngBwANqSTJwvHjrRgVa7f5xhcOH97D0mgQhUWhi44/Sn4kpFGmE428hoBqFeKxnCV\nx9o3fJgec2z1Ixp6Cu3VCBarIc/KdcMyzBjttKi3U+Ih48KMxbbUtx1coPfH8P4t\nke3eJbLUQbhFNMAvxNLJm5J8HPD+iHn9xZ/lfAlJg0fYoodW47owGtVLIG9wShE8\njPhgOiL8KM2iOKRnxmPBqjyztxTC9LfaVHftsp+qimKIStLzqKYs825a8Ln/69k/\nuXs6bfJ5AgMBAAECggEAXrDwkaLyaUfcVdfGq9oJNjsH0x88PdDHUtgCtMCv+R3p\n/RHvqvhMlAAgR8qpKfg2fVSGhdNl+TcuxAJeTTA14jbNP28B9gbpo9HgKRpFAVhp\nwCR0nj6BxJm4NqPU3FSdHWO8C8tOvddnkKDugB55hItlsI9XJLD7YCk5Nfo8YoZg\nonCZ1pfiodWlF1ODmvvcRKelVxmq/nq+kau00hzE5E/oatUaxp9nRLWy6kKxXHSS\njOEfp5648HC0h9T7jyKuh9fz/Kwu762PN9yuvCkT41v5cGHmDSU8F7Cpze4lsujn\nBPHcbM3Dy6pWbLYf/AZS4b536lWPPCLt7N4YX44t6wKBgQDUGWYv5PssjGDMWWAX\n1vXfmYXaTZ/GjjDqYEakQtWyfWRfhuw8JcL3m8E5AzNrd0n5Ua1EkjlYMRnN1cnX\nxpkT4KyLVoJlqO0SboeJRGfdcg01cK+8gR6TAeNxDNZUKXMs2K2oIiCcUXldDOC4\nzKS/zNDI0euDYKJjzdQQB81/QwKBgQD+yKW9x7WPZbZsBcB86kyY1rrVjMgpysY/\nGlONKhQESIZAmBgudb+2MhBBPQ6sQ6QHPraeYOXwU5k6DjA6P9mbMuCumuKQdO8e\nnq+Qdh6u60SHIssutqaM+vdpc6ZNuWKGpkNQ6RoFdc9pJxCPqiHVj1eiCmy7jAlh\nhd85rcc1kwKBgGCrSuXdaZV+TcgVnDSBZwMIh/ddlcr7l9PbGZdYVunJOOxJJHat\nh1Uvijuf99mtPuwBjAVK2BQYHPixzhMokzB6IGpJfZk0Cwlxkxi/Ms+9YcPPnmk+\nOF8sZytpfG5TjgkMoOmW1GIqyDPe4LWZoCVDBo3hD4SLkLgIYfUEeWbdAoGBAMPt\nzPc6gva5QyWi7rmg/DJ0vszGLI9m7jlMp3/hIe+BTiySSdZAmqMjsLhamdQDWDpy\nEcGNxoWf5C9vBctlmpPs9jePyDdKlTA428395+6f4d0B9rn/n+i+flDLrNIMhCYD\n6bRo7afRRyUX83ZnYQl35+4mkiNPt08Vwen7JzydAoGBAJk81tA/zUPApnXtFQ4f\nKf5ur6lbWh4Td7Ap1Dhe0ZPUIj8nQAuLEr+cANQzTAB96esR6eTuHb33Ql5WDHpl\nfq0Xv2oChWrzBX1ZNpl9katNFoTWZxaQPeWVsidwTm7F4wYPr9ejkOZar3nNn7tq\nXo3jCNqWaV3t6nLZS1z++qUy\n-----END PRIVATE KEY-----\n"

```

После создания образа удаляем сеть и подсеть из яндекс облака и переходим к разворачиванию ВМ при помощи terraform:  

Для этого будем использовать сервисный аккаунт к яндекс-облаку с ролью "editor" и ключ к нем

![img_2.png](img_2.png) созданный сервисный аккаунт в веб-интерфейсе

Далее копируем терраформовские файлы настроек, yml-файлы ансибла, проверяем, что все готово к раскатке стека сервисов:  

```bash
valyan@valyan-pc:~/terraform$ terraform validate
Success! The configuration is valid.
valyan@valyan-pc:~/terraform$ terraform init
valyan@valyan-pc:~/terraform$ terraform plan


............
Plan: 13 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + external_ip_address_node01 = (known after apply)
  + external_ip_address_node02 = (known after apply)
  + external_ip_address_node03 = (known after apply)
  + external_ip_address_node04 = (known after apply)
  + external_ip_address_node05 = (known after apply)
  + external_ip_address_node06 = (known after apply)
  + internal_ip_address_node01 = "192.168.101.11"
  + internal_ip_address_node02 = "192.168.101.12"
  + internal_ip_address_node03 = "192.168.101.13"
  + internal_ip_address_node04 = "192.168.101.14"
  + internal_ip_address_node05 = "192.168.101.15"
  + internal_ip_address_node06 = "192.168.101.16"

```

terraform validate показал, что с конфигурацией все ок  
terraform plan показал, что выполниться в ходе запуска  
Весь вывод terraform plan прикладывать не стал (там очень много текста)

запускаем сборку стека сервисов командой terraform apply -auto-approve и пьем кофе

Долгий процесс создания кластера серверов завершился спустя  почти 6 минут успешно, вывод окончания процесса ниже:
```bash
null_resource.sync: Creation complete after 5m42s [id=511236848793978262]
null_resource.monitoring: Creating...
null_resource.monitoring: Provisioning with 'local-exec'...
null_resource.monitoring (local-exec): Executing: ["/bin/sh" "-c" "ANSIBLE_FORCE_COLOR=1 ansible-playbook -i ../ansible/inventory ../ansible/swarm-deploy-stack.yml --limit=managers"]

null_resource.monitoring (local-exec): PLAY [nodes] *******************************************************************

null_resource.monitoring (local-exec): TASK [Gathering Facts] *********************************************************
null_resource.monitoring (local-exec): ok: [node03.netology.yc]
null_resource.monitoring (local-exec): ok: [node02.netology.yc]
null_resource.monitoring (local-exec): ok: [node01.netology.yc]

null_resource.monitoring (local-exec): TASK [Check Current Leader] ****************************************************
null_resource.monitoring: Still creating... [10s elapsed]
null_resource.monitoring (local-exec): ok: [node02.netology.yc]
null_resource.monitoring (local-exec): ok: [node01.netology.yc]
null_resource.monitoring (local-exec): ok: [node03.netology.yc]

null_resource.monitoring (local-exec): TASK [Run deploy, if node is leader] *******************************************
null_resource.monitoring (local-exec): skipping: [node02.netology.yc]
null_resource.monitoring (local-exec): skipping: [node03.netology.yc]
null_resource.monitoring: Still creating... [20s elapsed]
null_resource.monitoring: Still creating... [30s elapsed]
null_resource.monitoring: Still creating... [40s elapsed]
null_resource.monitoring (local-exec): changed: [node01.netology.yc]

null_resource.monitoring (local-exec): PLAY RECAP *********************************************************************
null_resource.monitoring (local-exec): node01.netology.yc         : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
null_resource.monitoring (local-exec): node02.netology.yc         : ok=2    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
null_resource.monitoring (local-exec): node03.netology.yc         : ok=2    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0

null_resource.monitoring: Creation complete after 48s [id=645498518791136677]

Apply complete! Resources: 13 added, 0 changed, 0 destroyed.

Outputs:

external_ip_address_node01 = "51.250.71.30"
external_ip_address_node02 = "51.250.89.160"
external_ip_address_node03 = "51.250.84.51"
external_ip_address_node04 = "51.250.76.140"
external_ip_address_node05 = "51.250.83.161"
external_ip_address_node06 = "51.250.78.115"
internal_ip_address_node01 = "192.168.101.11"
internal_ip_address_node02 = "192.168.101.12"
internal_ip_address_node03 = "192.168.101.13"
internal_ip_address_node04 = "192.168.101.14"
internal_ip_address_node05 = "192.168.101.15"
internal_ip_address_node06 = "192.168.101.16"
valyan@valyan-pc:~/terraform$ 
```
По итогам создалось 6 ВМ: 
![img_3.png](img_3.png) созданные терраформом объекты в яндекс-облаке  

![img_4.png](img_4.png) список ВМ  

![img_5.png](img_5.png) список ip адресов ВМ  



![img_6.png](img_6.png)

Подключаемся к первой ноде, которая является лидером после обработки списка нод алгоритмом RAFT и выводим список нод командой docker node ls:  

```bash
valyan@valyan-pc:~$ ssh centos@51.250.71.30 
[centos@node01 ~]$ sudo -i
[root@node01 ~]# docker node ls
ID                            HOSTNAME             STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
hmz67avv2zq2k8x5evtpdupht *   node01.netology.yc   Ready     Active         Leader           20.10.16
ae7z0cyzpai9sq2d1l8angeoo     node02.netology.yc   Ready     Active         Reachable        20.10.16
i6h87lxbc8bajsmbkj4lkk6mj     node03.netology.yc   Ready     Active         Reachable        20.10.16
ff4aubddb6dgtgfyowc4xuh59     node04.netology.yc   Ready     Active                          20.10.16
jedbjmsn6izj2bphltl37l8e0     node05.netology.yc   Ready     Active                          20.10.16
ozoc1k4g0hawgvqhiq5163tjn     node06.netology.yc   Ready     Active                          20.10.16
[root@node01 ~]# 
```


## Задача 3
Подключаемся к ноде1 и выводим содержание кластера мониторинга 

```bash
valyan@valyan-pc:~$ ssh centos@51.250.71.30 
[centos@node01 ~]$ sudo -i
[root@node01 ~]# docker node ls
ID                            HOSTNAME             STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
hmz67avv2zq2k8x5evtpdupht *   node01.netology.yc   Ready     Active         Leader           20.10.16
ae7z0cyzpai9sq2d1l8angeoo     node02.netology.yc   Ready     Active         Reachable        20.10.16
i6h87lxbc8bajsmbkj4lkk6mj     node03.netology.yc   Ready     Active         Reachable        20.10.16
ff4aubddb6dgtgfyowc4xuh59     node04.netology.yc   Ready     Active                          20.10.16
jedbjmsn6izj2bphltl37l8e0     node05.netology.yc   Ready     Active                          20.10.16
ozoc1k4g0hawgvqhiq5163tjn     node06.netology.yc   Ready     Active                          20.10.16
```

Вывод запущенных на кластере сервисов:  
```bash
[root@node01 ~]# docker service ls
ID             NAME                                MODE         REPLICAS   IMAGE                                          PORTS
hz9iatbwr4wy   swarm_monitoring_alertmanager       replicated   1/1        stefanprodan/swarmprom-alertmanager:v0.14.0    
oas9968726n7   swarm_monitoring_caddy              replicated   1/1        stefanprodan/caddy:latest                      *:3000->3000/tcp, *:9090->9090/tcp, *:9093-9094->9093-9094/tcp
owyliasb6lzs   swarm_monitoring_cadvisor           global       6/6        google/cadvisor:latest                         
mafb6o7lrrz8   swarm_monitoring_dockerd-exporter   global       6/6        stefanprodan/caddy:latest                      
pdwshdt819zg   swarm_monitoring_grafana            replicated   1/1        stefanprodan/swarmprom-grafana:5.3.4           
16wuk7dfl6by   swarm_monitoring_node-exporter      global       6/6        stefanprodan/swarmprom-node-exporter:v0.16.0   
fchenx85u5y0   swarm_monitoring_prometheus         replicated   1/1        stefanprodan/swarmprom-prometheus:v2.5.0       
zzxyn32krk5d   swarm_monitoring_unsee              replicated   1/1        cloudflare/unsee:v0.8.0                        
```

Вывод краткого содержания и имени кластера:
```bash
[root@node01 ~]# docker stack ls
NAME               SERVICES   ORCHESTRATOR
swarm_monitoring   8          Swarm
```

Вывод процессов, запущенных в стеке мониторинга:  
```bash
[root@node01 ~]# docker stak ps swarm_monitoring
docker: 'stak' is not a docker command.
See 'docker --help'
[root@node01 ~]# docker stack ps swarm_monitoring
ID             NAME                                                          IMAGE                                          NODE                 DESIRED STATE   CURRENT STATE             ERROR     PORTS
aw50anluoyg2   swarm_monitoring_alertmanager.1                               stefanprodan/swarmprom-alertmanager:v0.14.0    node01.netology.yc   Running         Running 12 minutes ago              
iaud5zx9zxzd   swarm_monitoring_caddy.1                                      stefanprodan/caddy:latest                      node01.netology.yc   Running         Running 12 minutes ago              
iul6rhsdy600    \_ swarm_monitoring_caddy.1                                  stefanprodan/caddy:latest                      node01.netology.yc   Shutdown        Complete 12 minutes ago             
y18ob6pn2aad   swarm_monitoring_cadvisor.ae7z0cyzpai9sq2d1l8angeoo           google/cadvisor:latest                         node02.netology.yc   Running         Running 12 minutes ago              
khtcaon5tjq2   swarm_monitoring_cadvisor.ff4aubddb6dgtgfyowc4xuh59           google/cadvisor:latest                         node04.netology.yc   Running         Running 13 minutes ago              
x3qoy0xnk1e2   swarm_monitoring_cadvisor.hmz67avv2zq2k8x5evtpdupht           google/cadvisor:latest                         node01.netology.yc   Running         Running 12 minutes ago              
76dqi6kv1rnz   swarm_monitoring_cadvisor.i6h87lxbc8bajsmbkj4lkk6mj           google/cadvisor:latest                         node03.netology.yc   Running         Running 12 minutes ago              
wsp1xjxp6qga   swarm_monitoring_cadvisor.jedbjmsn6izj2bphltl37l8e0           google/cadvisor:latest                         node05.netology.yc   Running         Running 13 minutes ago              
sxvk93j9srhq   swarm_monitoring_cadvisor.ozoc1k4g0hawgvqhiq5163tjn           google/cadvisor:latest                         node06.netology.yc   Running         Running 13 minutes ago              
s2elj23xukvy   swarm_monitoring_dockerd-exporter.ae7z0cyzpai9sq2d1l8angeoo   stefanprodan/caddy:latest                      node02.netology.yc   Running         Running 13 minutes ago              
7i0htcug44l6   swarm_monitoring_dockerd-exporter.ff4aubddb6dgtgfyowc4xuh59   stefanprodan/caddy:latest                      node04.netology.yc   Running         Running 13 minutes ago              
yxucib7g7lio   swarm_monitoring_dockerd-exporter.hmz67avv2zq2k8x5evtpdupht   stefanprodan/caddy:latest                      node01.netology.yc   Running         Running 13 minutes ago              
rsyvbjsmddqe   swarm_monitoring_dockerd-exporter.i6h87lxbc8bajsmbkj4lkk6mj   stefanprodan/caddy:latest                      node03.netology.yc   Running         Running 12 minutes ago              
zwws9l450off   swarm_monitoring_dockerd-exporter.jedbjmsn6izj2bphltl37l8e0   stefanprodan/caddy:latest                      node05.netology.yc   Running         Running 13 minutes ago              
c31wchgpk6wd   swarm_monitoring_dockerd-exporter.ozoc1k4g0hawgvqhiq5163tjn   stefanprodan/caddy:latest                      node06.netology.yc   Running         Running 13 minutes ago              
3e6c2zas3psi   swarm_monitoring_grafana.1                                    stefanprodan/swarmprom-grafana:5.3.4           node02.netology.yc   Running         Running 12 minutes ago              
vwomnjqvj8mr   swarm_monitoring_node-exporter.ae7z0cyzpai9sq2d1l8angeoo      stefanprodan/swarmprom-node-exporter:v0.16.0   node02.netology.yc   Running         Running 12 minutes ago              
eci28oihqqqt   swarm_monitoring_node-exporter.ff4aubddb6dgtgfyowc4xuh59      stefanprodan/swarmprom-node-exporter:v0.16.0   node04.netology.yc   Running         Running 12 minutes ago              
ye79lqyoakkf   swarm_monitoring_node-exporter.hmz67avv2zq2k8x5evtpdupht      stefanprodan/swarmprom-node-exporter:v0.16.0   node01.netology.yc   Running         Running 12 minutes ago              
vc0a67xafkw1   swarm_monitoring_node-exporter.i6h87lxbc8bajsmbkj4lkk6mj      stefanprodan/swarmprom-node-exporter:v0.16.0   node03.netology.yc   Running         Running 12 minutes ago              
x3hjecacaqxe   swarm_monitoring_node-exporter.jedbjmsn6izj2bphltl37l8e0      stefanprodan/swarmprom-node-exporter:v0.16.0   node05.netology.yc   Running         Running 12 minutes ago              
n1br81j25aaa   swarm_monitoring_node-exporter.ozoc1k4g0hawgvqhiq5163tjn      stefanprodan/swarmprom-node-exporter:v0.16.0   node06.netology.yc   Running         Running 12 minutes ago              
wntog1vxjvog   swarm_monitoring_prometheus.1                                 stefanprodan/swarmprom-prometheus:v2.5.0       node03.netology.yc   Running         Running 13 minutes ago              
tljvrrdpnxzg   swarm_monitoring_unsee.1                                      cloudflare/unsee:v0.8.0                        node04.netology.yc   Running         Running 12 minutes ago              
[root@node01 ~]# 
```

Далее проверяем настроенную отказоустойчивость:  
Подключаемся одновременно к первой ноде (лидеру) и ко второй ноде, видно, что отметка Leader установлена напротив Ноды1 в списке:  
![img_7.png](img_7.png)

Перезагружаем ноду 1  командой shutwodn -r now и смотрим, что происходит с кластером:
![img_8.png](img_8.png)
Как видно, после перезагрузки лидер-ноды роль лидера перешла к Ноде 2


В качестве дополнительной поверки посмотрим, что находится по адресу 51.250.89.160:3000
![img_9.png](img_9.png)
Т.е у нас настроена рабочая графана с несколькими дашбордами:


![img_10.png](img_10.png) мониторинг основных параметров нод  

![img_11.png](img_11.png) мониторинг запущенных на нодах сервисов

![img_12.png](img_12.png) мониторинг основных параметров самого кластера


## Задача 4 (*)
Выполнить на лидере Docker Swarm кластера команду (указанную ниже) и дать письменное описание её функционала, что она делает и зачем она нужна:


```bash

[root@node02 ~]# docker node ls
ID                            HOSTNAME             STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
hmz67avv2zq2k8x5evtpdupht     node01.netology.yc   Ready     Active         Reachable        20.10.16
ae7z0cyzpai9sq2d1l8angeoo *   node02.netology.yc   Ready     Active         Leader           20.10.16
i6h87lxbc8bajsmbkj4lkk6mj     node03.netology.yc   Ready     Active         Reachable        20.10.16
ff4aubddb6dgtgfyowc4xuh59     node04.netology.yc   Ready     Active                          20.10.16
jedbjmsn6izj2bphltl37l8e0     node05.netology.yc   Ready     Active                          20.10.16
ozoc1k4g0hawgvqhiq5163tjn     node06.netology.yc   Ready     Active                          20.10.16
[root@node02 ~]# docker swarm update --autolock=true
Swarm updated.
To unlock a swarm manager after it restarts, run the `docker swarm unlock`
command and provide the following key:

    SWMKEY-1-TjkAij41tVhkmBo8yd0aTTOSYrqFu6zDT5SqLX4OG2M

Please remember to store this key in a password manager, since without it you
will not be able to restart the manager.
[root@node02 ~]# 
```
Команда предназначена для блокировки менеджеров кластера docker swarm с шифрованием.. После блокировки не будут перезапускаться, включаться или выключаться объекты в кластере (ноды, сервисы, контейнеры). Генерится ключ, при помощи которого можно будет разблокировать swarm для  перезагрузки, включения, выключения какого либо из контейнеров, сервисов или нод.


Завершаем выполнение удалив все из яндекс-облака (чтоб денюшку не кушало):  

Удаляем кластер со всеми настройками:  
```bash
valyan@valyan-pc:~/terraform$ terraform destroy --auto-approve
yandex_vpc_network.default: Refreshing state... [id=enp0gu1cl0luvoaee95d]
yandex_vpc_subnet.default: Refreshing state... [id=e9bo1edlnet5rk8aftu9]
yandex_compute_instance.node03: Refreshing state... [id=fhm6m6huo93h0cmf0e4r]
yandex_compute_instance.node02: Refreshing state... [id=fhmg7u9qm64ecvn8h7ol]
yandex_compute_instance.node01: Refreshing state... [id=fhmspmi3cu9mls84847q]
yandex_compute_instance.node05: Refreshing state... [id=fhm0s4lglsgf6lhup6vu]
yandex_compute_instance.node04: Refreshing state... [id=fhmle0ikrtdfrnte9u0m]
yandex_compute_instance.node06: Refreshing state... [id=fhmo3p2vjo7ip22kre0e]
local_file.inventory: Refreshing state... [id=c45124cf73fc65bb236eefd0506b091bd9ee42af]
null_resource.wait: Refreshing state... [id=8726165359497011438]
null_resource.cluster: Refreshing state... [id=9223076744373321302]
null_resource.sync: Refreshing state... [id=511236848793978262]
null_resource.monitoring: Refreshing state... [id=645498518791136677]

Destroy complete! Resources: 13 destroyed.
valyan@valyan-pc:~/terraform$
```

Удаляем созданный образ:  
```bash
valyan@valyan-pc:~/terraform$ cd
valyan@valyan-pc:~$ yc compute image list
+----------------------+---------------+--------+----------------------+--------+
|          ID          |     NAME      | FAMILY |     PRODUCT IDS      | STATUS |
+----------------------+---------------+--------+----------------------+--------+
| fd8bg7u7kjpqun35p7a1 | centos-7-base | centos | f2e99agij1uhrk2ioilk | READY  |
+----------------------+---------------+--------+----------------------+--------+

valyan@valyan-pc:~$ yc compute image delete --id fd8bg7u7kjpqun35p7a1
done (10s)
valyan@valyan-pc:~$ yc compute image list
+----+------+--------+-------------+--------+
| ID | NAME | FAMILY | PRODUCT IDS | STATUS |
+----+------+--------+-------------+--------+
+----+------+--------+-------------+--------+

valyan@valyan-pc:~$ 
```